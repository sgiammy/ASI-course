{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 style=\"text-align:center\">Advanced Statistical Inference</h1>\n",
    "<h1 style=\"text-align:center\">Gaussian Process Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Aims\n",
    "<div class=\"alert alert-info\">\n",
    "<ul> \n",
    "<li>To sample from a Gaussian process prior distribution.\n",
    "<li>To implement Gaussian process inference for regression.\n",
    "<li>To use the above to observe samples from a Gaussian process posterior distribution.\n",
    "<li>To evaluate how different hyperparameter settings impact model quality.\n",
    "<li> To investigate different kernel functions and parameter optimisation strategies.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Introduction\n",
    "<div class=\"alert alert-info\">\n",
    "Gaussian processes (henceforth GPs) achieve greater flexibility over parametric models by imposing a preference bias as opposed to restrictive constraints. Although the parameterisation of GPs allows one to access a certain (infinite) set of functions, preference can be expressed using a prior over functions. This allows greater freedom in representing data dependencies, thus enabling the construction of better-suited models. In this lab, we shall cover the basic concepts of GP regression. For the sake of clarity, we shall focus on univariate data, which allows for better visualisation of the GP model. Nonetheless, the code implemented within this lab can be very easily extended to handle\n",
    "multi-dimensional inputs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Sampling from the GP Prior\n",
    "<div class=\"alert alert-info\">\n",
    "We shall consider a one-dimensional regression problem, whereby the inputs x are transformed by\n",
    "a function $f(x) = sin(exp(0.03 * x))$. \n",
    "<br><br>\n",
    "Generate 200 random points, $x$, in the range $[-20, 80]$, and compute their corresponding function\n",
    "values, $t$. The target function can then be plotted accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TO COMPLETE\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Recall that since GPs are non-parametric; we define a prior distribution over functions (models),\n",
    "specified as a multivariate Gaussian distribution $p(f) = N (\\mu, \\Sigma)$.\n",
    "\n",
    "Without loss of generality, we shall assume a zero-mean GP prior, i.e. $\\mu = 0$. The covariance\n",
    "matrix of the distribution, $\\Sigma$, may then be computed by evaluating the covariance between the\n",
    "input points. For this tutorial, we shall consider the widely used squared-exponential (RBF)\n",
    "covariance (also referred to as the kernel function), which is defined between two points as: \n",
    "\n",
    "$$k(x, x') = \\sigma_f^2 \\exp \\Big( -\\dfrac {(x-x')^2}{2l^2} \\Big). $$\n",
    "\n",
    "This kernel is parameterised by a lengthscale parameter $l$, and variance $\\sigma_f^2$ . Given that the true\n",
    "function may be assumed to be corrupted with noise, we can also add a noise parameter, $\\sigma_n^2$ , to\n",
    "the diagonal entries of the resulting kernel matrix, $K$, such that\n",
    "\n",
    "$$K_y = K + \\sigma_n^2I.$$\n",
    "<br>\n",
    "Complete the `compute_kernel()` function for computing the RBF kernel $K$ between two sets of input points.<br><br>\n",
    "\n",
    "Hint: The ‘cdist’ function in scipy can be used for evaluating the pairwise Euclidean distance between two sets of points.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_kernel(x1, x2, lengthscale, variance):\n",
    "\n",
    "    '''\n",
    "    \n",
    "    TO COMPLETE\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Assuming a zero-mean prior, and using the kernel matrix constructed with `compute_kernel()` for input points x, we can sample from the prior distribution using the numpy `multivariate_normal()` function.<br><br>\n",
    "\n",
    "For the time being, you can initialise the kernel parameters as follows:\n",
    "<br>\n",
    "- lengthscale = 10<br>\n",
    "- variance = 0.1\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mu = ... # TO COMPLETE\n",
    "K = ... # TO COMPLETE\n",
    "R = np.random.multivariate_normal(...) # TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "On the same figure where we plotted the true function, draw four sample functions from the prior distribution and plot them accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TO COMPLETE\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    " In order to better understand the role of the hyperparameters, observe how altering the kernel parameters impacts the shape of the prior samples.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TO COMPLETE\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. GP Inference\n",
    "<div class=\"alert alert-info\">\n",
    "Recall that the prior represents our prior beliefs before observing the function values of any data points. Suppose we can now observe 3 points at random from the input data; we would expect that with this additional knowledge, the functions drawn from the updated GP distribution would be constrained to pass through these points (or at least close if corrupted with noise). The combination of the prior and the likelihood of the observed data leads to the posterior distribution over functions.\n",
    "<br><br>\n",
    "Assign 3 points at random from $x$ (and their corresponding function values) to `obs_x` and `obs_t`\n",
    "respectively. For now we shall assume that all other $x$ values are unobserved.<br><br>\n",
    "\n",
    "You are encouraged to use the given initial configuration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lengthscale = 3\n",
    "variance = 2\n",
    "noise = 1e-3\n",
    "\n",
    "params = [lengthscale, variance, noise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "numObs = 10\n",
    "obs_x = np.random.choice(x, numObs, replace=False)\n",
    "obs_t = ... # TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Complete the provided implementation of `gp_inference` for evaluating the posterior GP mean and variance using the equations given in the lecture.\n",
    "<br><br>\n",
    "<b>Note</b>: As we have encountered in previous labs, matrix inversions can be both numerically troublesome and slow to compute. In this lab, we shall avoid computing matrix inversions directly by instead considering Cholesky decompositions for solving linear systems. You are encouraged to read more about Cholesky decompositions for GPs by consulting Appendix A.4 of <a target=\"_blank\" href=\"http://www.gaussianprocess.org/gpml/\">Gaussian Processes for Machine Learning (Rasmussen and Williams, 2005)</a> - available online!<br><br>\n",
    "\n",
    "The complete pseudo-code for the following procedure is provided in Algorithm 2.1 from Chapter 2 of this same book.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gp_inference(obs_x, obs_t, x_new, params):\n",
    "    \n",
    "    # unpack params\n",
    "    lengthscale = params[0]\n",
    "    variance = params[1]\n",
    "    noise = params[2]\n",
    "    \n",
    "    N = np.shape(obs_x)[0]\n",
    "    \n",
    "    # compute kernel\n",
    "    K = ... \n",
    "    K_y = ...\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    When computing the posterior mean, we would like to avoid evaluating\n",
    "        \n",
    "                            alpha = np.linalg.inv(K_y).dot(obs_t) \n",
    "    \n",
    "    directly. The Cholesky decomposition can be applied using the following procedure.\n",
    "    \n",
    "        -> Compute the lower triangular Cholesky decomposition of K_y (which we shall call K_chol)\n",
    "        -> Compute 'alpha' as:\n",
    "        \n",
    "                            alpha = K_chol.T \\ (K_chol \\ obs_t)\n",
    "                            \n",
    "           where the back-substitution operator can be evaluated using the 'solve_triangular' \n",
    "           function in scipy. Make sure to set the function's lower' flag as appropriate.\n",
    "            \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # compute the Cholesky decomposition of K_y\n",
    "    K_chol = ...\n",
    "    \n",
    "    # compute alpha\n",
    "    alpha = ...\n",
    "    \n",
    "    # compute the covariance between the training and test data\n",
    "    K_obs_pred = ...\n",
    "    \n",
    "    # compute the covariance for the test data\n",
    "    K_pred = ...\n",
    "    \n",
    "    # compute the posterior mean\n",
    "    posterior_m = np.dot(K_obs_pred.T, alpha)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Similarly, when computing\n",
    "    \n",
    "                        v = np.linalg.inv(kern_obs)*kern_obs_pred\n",
    "                        \n",
    "    employ the Cholesky decomposition as outlined above.\n",
    "                        \n",
    "    '''\n",
    "    \n",
    "    v = ...\n",
    "    posterior_v = ...\n",
    "    \n",
    "    \n",
    "    return posterior_m, posterior_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Sampling from the  GP Posterior<br>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Now that you have computed the posterior mean and variance, create a new figure once again showing the true function. To this figure, add the posterior mean and uncertainty (show two standard deviations) evaluated on the same $x$ values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "posterior_m, posterior_v = gp_inference(...) # TO COMPLETE\n",
    "R2 = np.random.multivariate_normal(...) # TO COMPLETE\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "The variance at each point is given by the diagonal of the variance matrix.\n",
    "Recall that the standard deviation is the square root of the variance.\n",
    "\n",
    "'''\n",
    "\n",
    "posterior_std = ... # TO COMPLETE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Plot the results using error bars in order to display the posterior uncertainty.\n",
    "\n",
    "TO COMPLETE\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note that we should also add the noise variance to the predictive variance of the posterior. Fix this accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TO COMPLETE\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Additionally, as we did with the prior distribution, sample four functions from the posterior and plot them on the same figure. Comment on the obtained plots. You should also view how the posterior mean and variance change when more observations are treated as observations in the GP inference procedure.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TO COMPLETE\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "As a measure of model quality, you should also compute the log marginal likelihood of the model.\n",
    "To this end, complete the code provided in `gp inference()` to include the negative log likelihood term.\n",
    "<br><br>\n",
    "You could attempt a grid search over a range of parameter values in order to determine which configuration yields the best result.<br><br>\n",
    "\n",
    "Hint: Once again, refer to Algorithm 2.1 in Chapter 2 of the book referenced above for details on how to compute this term\n",
    "efficiently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TO COMPLETE\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bonus: Alternative Kernel Functions\n",
    "<div class=\"alert alert-info\">\n",
    "In this notebook, we have focused exclusively on the RBF kernel.\n",
    "However, the choice of kernel function (along with its associated parameters) can have a significant effect on the overall Gaussian process model.\n",
    "Choosing the best kernel to fit your data is no simple task, and is a pertinent problem in many applied domains.<br>\n",
    "\n",
    "A brief discussion on this problem may be found here: <a target=\"_blank\" href=\"https://www.cs.toronto.edu/~duvenaud/cookbook/\">Kernel Cookbook</a>. \n",
    "\n",
    "Familiarise yourself better with these issues by implementing one or two additional kernels.\n",
    "The effectiveness of each kernel may then be easily evaluated by simply overriding the previously defined `compute_kernel()` function.\n",
    "As a further step, you could also extend this comparison to other test functions besides the one being used in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TO COMPLETE\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bonus: Parameter Optimisation using Gradient Descent\n",
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "Optimise the hyperparameters of the model by minimising the negative log-likelihood of the model. For a complete solution, you should include the derivatives of the objective function with respect to the parameters being optimised.\n",
    "<br><br>\n",
    "The general formula for computing the derivative is given below:<br>\n",
    "$$\n",
    "\\frac{\\partial\\;\\text{NLL}}{\\partial\\;\\theta_i} = - \\frac{1}{2} \\textbf{Tr} \\left( K^{-1} \\frac{\\partial K}{\\partial \\theta_i} \\right) + \\frac{1}{2} \\textbf{y}^{T} K^{-1} \\frac{\\partial K}{\\partial \\theta_i} K^{-1} \\textbf{y}.\n",
    "$$<br>\n",
    "To give a more concrete example, the $\\frac{\\partial K}{\\partial \\theta_i}$ term for the lengthscale parameter in the RBF kernel is computed as follows:\n",
    "$$\n",
    "\\frac{\\partial K}{\\partial l} = \\sigma_f^2 \\exp \\left( -\\dfrac {(x-x')^2}{2l^2} \\right)\\left( \\dfrac {(x-x')^2}{l^3} \\right)\n",
    "$$\n",
    "<br><br>\n",
    "<b>Pro tip:</b> Note that the parameters $l$, $\\sigma_f^2$ , and $\\sigma_n^2$ are always expected to be positive. It is possible that the optimisation algorithm attempts to evaluate the log-likelihood in regions of the parameter space where one or more of these parameters are negative, leading to numerical issues. While not expected in this notebook, a commonly-used technique to enforce this condition is to work with a transformed version of covariance parameters using the logarithm transformation. In particular, define $\\psi_l = log(l)$, $\\psi_f = log(\\sigma_f^2 )$, and $\\psi_n = log(\\sigma_n^2 )$, and optimise with respect to the $\\psi$ parameters. The optimisation problem in the transformed space is now unbounded, and the gradient of the log-likelihood should be computed with respect to the $\\psi$ parameters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TO COMPLETE\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
